{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_MARKS  = [\"H3K27AC\", \"H3K27ME3\", \"H3K36ME3\", \"H3K4ME1\", \"H3K4ME3\", \"INPUT\"]\n",
    "OUTPUT_MARKS = [\"H3K27AC\"]\n",
    "MARK_INDEX   = [0, 1, 2, 3, 4, 5]\n",
    "PEAK_MARK_INDEX = [0, 1, 2, 3, 4]\n",
    "SEQ_LENGTH = 1001\n",
    "#DATA_PATH = './data/GM12878_5+1marks-K4me3_all_subsample-0.5e6-None_rS-0_numEx-1000_seqLen-1001_peakFrac-0.5_peaksFac-H3K27AC_norm-arcsinh.npz'\n",
    "DATA_PATH = 'GM12878.npz'\n",
    "EVAL_PATH = 'GM18526.npz'\n",
    "zero_out_non_bins = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "\n",
    "def input_not_before_end(list_of_marks):\n",
    "    return ('INPUT' not in list_of_marks[:-1])\n",
    "\n",
    "def load_seq_dataset():\n",
    "    seq_length = SEQ_LENGTH\n",
    "    input_marks = INPUT_MARKS\n",
    "    output_marks = OUTPUT_MARKS\n",
    "  \n",
    "    assert(input_not_before_end(output_marks))\n",
    "    assert(input_not_before_end(input_marks))\n",
    "    \n",
    "    dataset_path = os.path.join(DATA_PATH)\n",
    "\n",
    "    #try:      \n",
    "    with np.load(dataset_path) as data:\n",
    "        X = data['X'].astype('float32')\n",
    "        Y = data['Y'].astype('float32')\n",
    "        peakPValueX = data['peakPValueX'].astype('float32')\n",
    "        peakPValueY = data['peakPValueY'].astype('float32')\n",
    "        peakBinaryX = data['peakBinaryX'].astype('int8')\n",
    "        peakBinaryY = data['peakBinaryY'].astype('int8')\n",
    "    #except:\n",
    "        #raise Exception, \"Dataset doesn't exist or is missing a required matrix.\"\n",
    "\n",
    "    \n",
    "    marks_idx =  MARK_INDEX\n",
    "    peak_marks_idx = PEAK_MARK_INDEX\n",
    "    \n",
    "    X = X[..., marks_idx]\n",
    "    peakPValueX = peakPValueX[..., peak_marks_idx]\n",
    "    peakBinaryX = peakBinaryX[..., peak_marks_idx]\n",
    "\n",
    "    assert(np.all(peakPValueX >= 0) & np.all(peakPValueY >= 0))\n",
    "\n",
    "    if (X.shape[0], X.shape[1]) != (Y.shape[0], Y.shape[1]):\n",
    "        raise Exception(\"First two dimensions of X and Y shapes (num_examples, seq_length) \\\n",
    "                          need to agree.\")\n",
    "    if (peakPValueX.shape[0], peakPValueX.shape[1]) != (peakPValueY.shape[0], peakPValueY.shape[1]):\n",
    "        raise Exception(\"First two dimensions of peakPValueX and peakPValueY shapes \\\n",
    "                          (num_examples, seq_length) need to agree.\")\n",
    "    if len(peakPValueX) != len(X):\n",
    "        raise Exception(\"peakPValueX and X must have same length.\")\n",
    "\n",
    "    if ((seq_length != X.shape[1]) or (seq_length != peakPValueX.shape[1])):\n",
    "        raise Exception(\"seq_length between model and data needs to agree\")\n",
    "\n",
    "    return X, Y, peakPValueX, peakPValueY, peakBinaryX, peakBinaryY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, peakPValueX, peakPValueY, peakBinaryX, peakBinaryY = load_seq_dataset()\n",
    "\n",
    "if zero_out_non_bins:\n",
    "    peakPValueX = peakPValueX * peakBinaryX\n",
    "    peakPValueY = peakPValueY * peakBinaryY \n",
    "\n",
    "def sq2p_process_X(X):\n",
    "    return X\n",
    "\n",
    "def sq2p_process_Y(Y):\n",
    "    mid = (SEQ_LENGTH - 1) / 2\n",
    "    mid = int(mid)\n",
    "    return Y[:, mid:mid+1, :]    \n",
    "    \n",
    "X = sq2p_process_X(X)\n",
    "Y = sq2p_process_Y(Y)\n",
    "peakPValueX = sq2p_process_X(peakPValueX)\n",
    "peakPValueY = sq2p_process_Y(peakPValueY)\n",
    "peakBinaryX = sq2p_process_X(peakBinaryX)\n",
    "peakBinaryY = sq2p_process_Y(peakBinaryY)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataNormalizer(object):\n",
    "    def __init__(self, mode):\n",
    "        self.b = None\n",
    "        self.W = None\n",
    "        self.mode = mode\n",
    "        if mode not in ['ZCA', 'Z', '01', 'identity']:\n",
    "            raise ValueError(\"mode=%s must be 'ZCA', 'Z', '01', or 'identity'\" % mode)\n",
    "\n",
    "\n",
    "    def fit(self, X_orig):\n",
    "        \"\"\"\n",
    "        Learns scaling parameters on the X_orig dataset. Does not modify X_orig.\n",
    "        \"\"\"        \n",
    "        if len(X_orig.shape) != 2 and len(X_orig.shape) != 3:\n",
    "            raise ValueError(\"X must be either a 3-tensor of shape num_examples x seq_length x \\\n",
    "                               num_input_marks, or a 2-tensor of shape num_examples x num_input_marks\")\n",
    "        if self.mode == 'identity':\n",
    "            return None        \n",
    "\n",
    "        X = np.copy(X_orig)\n",
    "        num_input_marks = X.shape[-1]\n",
    "\n",
    "        # If X is a 3-tensor, reshape X such that it is a 2-tensor of shape \n",
    "        # (num_examples * seq_length) x num_input_marks. \n",
    "        if len(X.shape) == 3:    \n",
    "            X = np.reshape(X, (-1, num_input_marks))\n",
    "        \n",
    "        self.b = np.mean(X, axis=0) \n",
    "\n",
    "        X -= self.b\n",
    "\n",
    "        if self.mode == 'ZCA':\n",
    "            sigma = np.dot(X.T, X) / X.shape[0]\n",
    "            U, S, V = np.linalg.svd(sigma)\n",
    "            self.W = np.dot(\n",
    "                np.dot(U, np.diag(1 / np.sqrt(S + 1e-5))),\n",
    "                U.T)\n",
    "        elif self.mode == 'Z':\n",
    "            self.W = np.empty(num_input_marks)\n",
    "            for idx in range(num_input_marks):\n",
    "                self.W[idx] = np.std(X[:, idx])\n",
    "        elif self.mode == '01':\n",
    "            self.W = np.empty(num_input_marks)\n",
    "            for idx in range(num_input_marks):\n",
    "                self.W[idx] = np.max(np.abs(X[:, idx]))\n",
    "\n",
    "        return None            \n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        if len(X.shape) != 2 and len(X.shape) != 3:\n",
    "            raise ValueError(\"X must be either a 3-tensor of shape num_examples x seq_length x \\\n",
    "                               num_input_marks, or a 2-tensor of shape num_examples x num_input_marks\")\n",
    "\n",
    "        if self.mode == 'identity':\n",
    "            return X\n",
    "            \n",
    "        assert self.b is not None\n",
    "        assert self.W is not None\n",
    "\n",
    "        num_input_marks = X.shape[-1]\n",
    "        orig_shape = X.shape\n",
    "\n",
    "        if self.mode == 'ZCA':            \n",
    "            X = np.reshape(X, (-1, num_input_marks))\n",
    "            if self.W.shape[1] != X.shape[1]:\n",
    "                raise ValueError(\"When doing a ZCA transform, X and W must have the same number of columns.\")\n",
    "            X = np.dot(\n",
    "                X - self.b,\n",
    "                self.W.T)\n",
    "            X = np.reshape(X, orig_shape)\n",
    "        elif self.mode in ['Z', '01']:\n",
    "            if (len(self.b) != num_input_marks) or (len(self.W) != num_input_marks):\n",
    "                print(\"X.shape: \", X.shape)\n",
    "                print(\"b.shape: \", self.b.shape)\n",
    "                print(\"W.shape: \", self.W.shape)\n",
    "                raise ValueError(\"The shapes of X, b, and W must all share the same last dimension.\")                \n",
    "            for idx in range(num_input_marks):\n",
    "                X[..., idx] = (X[..., idx] - self.b[idx]) / self.W[idx]\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_input = \"01\"\n",
    "normalizer = DataNormalizer(scale_input)\n",
    "normalizer.fit(X)\n",
    "X = normalizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dense, Flatten\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers.convolutional import UpSampling1D\n",
    "\n",
    "NUM_FILTERS = 6\n",
    "FILTER_LEN  = 51\n",
    "def Seq2Point_model(isBinaryModel):\n",
    "    num_filters = NUM_FILTERS\n",
    "    filter_length = FILTER_LEN\n",
    "    seq_length = SEQ_LENGTH\n",
    "    num_output_marks = len(OUTPUT_MARKS)\n",
    "    num_input_marks = len(INPUT_MARKS)\n",
    "    \n",
    "    model = Sequential()\n",
    "    # border_mode='same' makes the length of the output \n",
    "    # the same size as the length of the input\n",
    "    # by adding just the right amount of zero padding to each side.\n",
    "    model.add(\n",
    "        Convolution1D(                    \n",
    "            num_filters, \n",
    "            filter_length,\n",
    "            input_dim=num_input_marks,\n",
    "            init='uniform', \n",
    "            border_mode='same')) \n",
    "\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # See below for documentation on border_mode='valid'\n",
    "    # We are essentially replicating the \"dense\" layer here, but with a convolutional layer\n",
    "    # so that later we can do genome-wide prediction.\n",
    "    model.add(\n",
    "        Convolution1D(\n",
    "            num_output_marks, # output_dim --> always 1                    \n",
    "            seq_length,\n",
    "            init='uniform',\n",
    "            border_mode='valid'))\n",
    "\n",
    "    if isBinaryModel: # one of these values is True and other is False\n",
    "        model.add(Activation('sigmoid')) # return 0 ~ 1. It means binary classification \n",
    "    else: \n",
    "        model.add(Activation('relu')) # return some value. It means signal regression\n",
    "\n",
    "    \n",
    "    print(model.summary())\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ncp/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ncp/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(6, 51, input_shape=(None, 6), padding=\"same\", kernel_initializer=\"uniform\")`\n",
      "/home/ncp/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(1, 1001, padding=\"valid\", kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, None, 6)           1842      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, None, 6)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 1)           6007      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, None, 1)           0         \n",
      "=================================================================\n",
      "Total params: 7,849\n",
      "Trainable params: 7,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, None, 6)           1842      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, None, 6)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, None, 1)           6007      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, None, 1)           0         \n",
      "=================================================================\n",
      "Total params: 7,849\n",
      "Trainable params: 7,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_sig = Seq2Point_model(isBinaryModel=False)\n",
    "model_bin = Seq2Point_model(isBinaryModel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2point_weight_sig  = 's2q-sig'\n",
    "sig_checkpointer = ModelCheckpoint(\n",
    "    filepath=os.path.join('.', '%s-weights.hdf5'%seq2point_weight_sig), \n",
    "    verbose=1, \n",
    "    save_best_only=True)\n",
    "\n",
    "seq2point_weight_bin = 's2q-bin'\n",
    "bin_checkpointer = ModelCheckpoint(\n",
    "    filepath=os.path.join('.', '%s-weights.hdf5'%seq2point_weight_bin), \n",
    "    verbose=1, \n",
    "    save_best_only=True)\n",
    "\n",
    "sig_earlystopper = EarlyStopping(monitor='val_loss', patience=3, verbose=0)\n",
    "bin_earlystopper = EarlyStopping(monitor='val_loss', patience=3, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCH          = 100\n",
    "VALID_SPLIT       = 0.1\n",
    "BATCH_SIZE        = 100\n",
    "# complie seq2point model\n",
    "model_sig.compile(loss='MSE', optimizer=\"Adam\")\n",
    "model_bin.compile(loss='binary_crossentropy', optimizer=\"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Point sig model fit start \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ncp/anaconda3/lib/python3.6/site-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "9000/9000 [==============================] - 2s 219us/step - loss: 0.7919 - val_loss: 0.3327\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.33273, saving model to ./s2q-sig-weights.hdf5\n",
      "Epoch 2/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 0.2698 - val_loss: 0.2892\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.33273 to 0.28924, saving model to ./s2q-sig-weights.hdf5\n",
      "Epoch 3/100\n",
      "9000/9000 [==============================] - 1s 63us/step - loss: 0.2527 - val_loss: 0.2849\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.28924 to 0.28492, saving model to ./s2q-sig-weights.hdf5\n",
      "Epoch 4/100\n",
      "9000/9000 [==============================] - 1s 63us/step - loss: 0.2486 - val_loss: 0.3286\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.28492\n",
      "Epoch 5/100\n",
      "9000/9000 [==============================] - 1s 63us/step - loss: 0.2395 - val_loss: 0.2846\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.28492 to 0.28463, saving model to ./s2q-sig-weights.hdf5\n",
      "Epoch 6/100\n",
      "9000/9000 [==============================] - 1s 62us/step - loss: 0.2387 - val_loss: 0.2833\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.28463 to 0.28329, saving model to ./s2q-sig-weights.hdf5\n",
      "Epoch 7/100\n",
      "9000/9000 [==============================] - 1s 63us/step - loss: 0.2348 - val_loss: 0.2812\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.28329 to 0.28120, saving model to ./s2q-sig-weights.hdf5\n",
      "Epoch 8/100\n",
      "9000/9000 [==============================] - 1s 63us/step - loss: 0.2325 - val_loss: 0.2792\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.28120 to 0.27920, saving model to ./s2q-sig-weights.hdf5\n",
      "Epoch 9/100\n",
      "9000/9000 [==============================] - 1s 61us/step - loss: 0.2280 - val_loss: 0.2784\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.27920 to 0.27844, saving model to ./s2q-sig-weights.hdf5\n",
      "Epoch 10/100\n",
      "9000/9000 [==============================] - 1s 63us/step - loss: 0.2343 - val_loss: 0.3061\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.27844\n",
      "Epoch 11/100\n",
      "9000/9000 [==============================] - 1s 63us/step - loss: 0.2246 - val_loss: 0.2979\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.27844\n",
      "Epoch 12/100\n",
      "9000/9000 [==============================] - 1s 65us/step - loss: 0.2227 - val_loss: 0.2844\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.27844\n",
      "Epoch 13/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 0.2172 - val_loss: 0.2788\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.27844\n",
      "Epoch 14/100\n",
      "9000/9000 [==============================] - 1s 65us/step - loss: 0.2222 - val_loss: 0.2853\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.27844\n",
      "Epoch 15/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.2173 - val_loss: 0.3081\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.27844\n",
      "Epoch 16/100\n",
      "9000/9000 [==============================] - 1s 65us/step - loss: 0.2234 - val_loss: 0.3118\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.27844\n",
      "Epoch 17/100\n",
      "9000/9000 [==============================] - 1s 65us/step - loss: 0.2118 - val_loss: 0.2863\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.27844\n",
      "Epoch 18/100\n",
      "9000/9000 [==============================] - 1s 65us/step - loss: 0.2137 - val_loss: 0.2887\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.27844\n",
      "Epoch 19/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 0.2261 - val_loss: 0.2824\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.27844\n",
      "Epoch 20/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.2133 - val_loss: 0.3283\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.27844\n",
      "Epoch 21/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 0.2097 - val_loss: 0.3116\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.27844\n",
      "Epoch 22/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 0.2043 - val_loss: 0.2892\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.27844\n",
      "Epoch 23/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 0.2037 - val_loss: 0.2919\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.27844\n",
      "Epoch 24/100\n",
      "9000/9000 [==============================] - 1s 63us/step - loss: 0.2035 - val_loss: 0.2874\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.27844\n",
      "Epoch 25/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 0.2013 - val_loss: 0.2903\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.27844\n",
      "Epoch 26/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 0.2004 - val_loss: 0.2945\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.27844\n",
      "Epoch 27/100\n",
      "9000/9000 [==============================] - 1s 63us/step - loss: 0.1972 - val_loss: 0.2934\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.27844\n",
      "Epoch 28/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 0.1984 - val_loss: 0.3503\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.27844\n",
      "Epoch 29/100\n",
      "9000/9000 [==============================] - 1s 63us/step - loss: 0.2030 - val_loss: 0.3014\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.27844\n",
      "Epoch 30/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 0.1918 - val_loss: 0.3052\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.27844\n",
      "Epoch 31/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.1909 - val_loss: 0.2916\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.27844\n",
      "Epoch 32/100\n",
      "9000/9000 [==============================] - 1s 63us/step - loss: 0.1912 - val_loss: 0.3069\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.27844\n",
      "Epoch 33/100\n",
      "9000/9000 [==============================] - 1s 65us/step - loss: 0.1906 - val_loss: 0.2998\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.27844\n",
      "Epoch 34/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 0.1995 - val_loss: 0.3120\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.27844\n",
      "Epoch 35/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 0.1975 - val_loss: 0.3637\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.27844\n",
      "Epoch 36/100\n",
      "9000/9000 [==============================] - 1s 65us/step - loss: 0.1913 - val_loss: 0.3040\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.27844\n",
      "Epoch 37/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.1903 - val_loss: 0.3164\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.27844\n",
      "Epoch 38/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.1864 - val_loss: 0.3144\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.27844\n",
      "Epoch 39/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 0.1848 - val_loss: 0.3207\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.27844\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# train seq2point model\n",
    "print('Seq2Point sig model fit start ')\n",
    "hist_sig = model_sig.fit(X, Y, \n",
    "                         callbacks=[sig_checkpointer, sig_earlystopper],\n",
    "                         nb_epoch=NB_EPOCH,\n",
    "                         validation_split=VALID_SPLIT,\n",
    "                         batch_size=BATCH_SIZE)\n",
    "print('----------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Point bin model fit start \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ncp/anaconda3/lib/python3.6/site-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "9000/9000 [==============================] - 1s 90us/step - loss: 0.2292 - val_loss: 0.1679\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16794, saving model to ./s2q-bin-weights.hdf5\n",
      "Epoch 2/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 0.1373 - val_loss: 0.1782\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.16794\n",
      "Epoch 3/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.1345 - val_loss: 0.1746\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.16794\n",
      "Epoch 4/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 0.1287 - val_loss: 0.1760\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.16794\n",
      "Epoch 5/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 0.1262 - val_loss: 0.1645\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.16794 to 0.16448, saving model to ./s2q-bin-weights.hdf5\n",
      "Epoch 6/100\n",
      "9000/9000 [==============================] - 1s 69us/step - loss: 0.1234 - val_loss: 0.1779\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.16448\n",
      "Epoch 7/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.1188 - val_loss: 0.1743\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.16448\n",
      "Epoch 8/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.1181 - val_loss: 0.1765\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.16448\n",
      "Epoch 9/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 0.1141 - val_loss: 0.1794\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.16448\n",
      "Epoch 10/100\n",
      "9000/9000 [==============================] - 1s 65us/step - loss: 0.1098 - val_loss: 0.1773\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.16448\n",
      "Epoch 11/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 0.1074 - val_loss: 0.1837\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.16448\n",
      "Epoch 12/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 0.1059 - val_loss: 0.1741\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.16448\n",
      "Epoch 13/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 0.1034 - val_loss: 0.1926\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.16448\n",
      "Epoch 14/100\n",
      "9000/9000 [==============================] - 1s 65us/step - loss: 0.1004 - val_loss: 0.1918\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.16448\n",
      "Epoch 15/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 0.0983 - val_loss: 0.1841\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.16448\n",
      "Epoch 16/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.0972 - val_loss: 0.1948\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.16448\n",
      "Epoch 17/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 0.0940 - val_loss: 0.1954\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.16448\n",
      "Epoch 18/100\n",
      "9000/9000 [==============================] - 1s 65us/step - loss: 0.0960 - val_loss: 0.2027\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.16448\n",
      "Epoch 19/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.0932 - val_loss: 0.2033\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.16448\n",
      "Epoch 20/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.0881 - val_loss: 0.2032\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.16448\n",
      "Epoch 21/100\n",
      "9000/9000 [==============================] - 1s 64us/step - loss: 0.0871 - val_loss: 0.2005\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.16448\n",
      "Epoch 22/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.0862 - val_loss: 0.2078\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.16448\n",
      "Epoch 23/100\n",
      "9000/9000 [==============================] - 1s 69us/step - loss: 0.0835 - val_loss: 0.2132\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.16448\n",
      "Epoch 24/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 0.0827 - val_loss: 0.2222\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.16448\n",
      "Epoch 25/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.0798 - val_loss: 0.2129\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.16448\n",
      "Epoch 26/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.0774 - val_loss: 0.2222\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.16448\n",
      "Epoch 27/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 0.0774 - val_loss: 0.2143\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.16448\n",
      "Epoch 28/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.0764 - val_loss: 0.2336\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.16448\n",
      "Epoch 29/100\n",
      "9000/9000 [==============================] - 1s 65us/step - loss: 0.0733 - val_loss: 0.2392\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.16448\n",
      "Epoch 30/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.0729 - val_loss: 0.2446\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.16448\n",
      "Epoch 31/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.0720 - val_loss: 0.2410\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.16448\n",
      "Epoch 32/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.0703 - val_loss: 0.2322\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.16448\n",
      "Epoch 33/100\n",
      "9000/9000 [==============================] - 1s 67us/step - loss: 0.0665 - val_loss: 0.2438\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.16448\n",
      "Epoch 34/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.0670 - val_loss: 0.2510\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.16448\n",
      "Epoch 35/100\n",
      "9000/9000 [==============================] - 1s 66us/step - loss: 0.0663 - val_loss: 0.2527\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.16448\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Seq2Point bin model fit start ')\n",
    "hist_bin = model_bin.fit(X, peakBinaryY, \n",
    "                         callbacks=[bin_checkpointer, bin_earlystopper],\n",
    "                         nb_epoch=NB_EPOCH,\n",
    "                         validation_split=VALID_SPLIT,\n",
    "                         batch_size=BATCH_SIZE)\n",
    "print('----------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
